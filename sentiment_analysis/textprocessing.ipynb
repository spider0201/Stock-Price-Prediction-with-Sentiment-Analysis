{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model vÃ  tokenizer\n",
    "checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "def predict_sentiment_score(text, max_length=256, stride=50):\n",
    "    \"\"\"\n",
    "    Dá»± Ä‘oÃ¡n Ä‘iá»ƒm cáº£m xÃºc cá»§a vÄƒn báº£n dÃ i báº±ng PhoBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): VÄƒn báº£n cáº§n phÃ¢n tÃ­ch.\n",
    "        max_length (int): Sá»‘ token tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh.\n",
    "        stride (int): Má»©c Ä‘á»™ chá»“ng láº¥n giá»¯a cÃ¡c Ä‘oáº¡n.\n",
    "\n",
    "    Returns:\n",
    "        dict: XÃ¡c suáº¥t cá»§a cÃ¡c nhÃ£n cáº£m xÃºc {label: probability}\n",
    "    \"\"\"\n",
    "    # Tokenize vÄƒn báº£n dÃ i, cáº¯t thÃ nh tá»«ng Ä‘oáº¡n\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    input_keys = [\"input_ids\", \"attention_mask\"]\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # KhÃ´ng tÃ­nh gradient Ä‘á»ƒ tÄƒng tá»‘c\n",
    "        for i in range(len(inputs[\"input_ids\"])):  # Duyá»‡t qua tá»«ng Ä‘oáº¡n Ä‘Ã£ cáº¯t\n",
    "            segment = {k: inputs[k][i].unsqueeze(0) for k in input_keys}\n",
    "            outputs = model(**segment)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_predictions.append(predictions[0].tolist())\n",
    "\n",
    "    # TÃ­nh trung bÃ¬nh dá»± Ä‘oÃ¡n tá»« táº¥t cáº£ cÃ¡c Ä‘oáº¡n\n",
    "    avg_predictions = torch.tensor(all_predictions).mean(dim=0)\n",
    "\n",
    "    # Chuyá»ƒn káº¿t quáº£ thÃ nh dict vá»›i nhÃ£n vÃ  xÃ¡c suáº¥t tÆ°Æ¡ng á»©ng\n",
    "    sentiment_scores = {\n",
    "        model.config.id2label[j]: avg_predictions[j].item()\n",
    "        for j in range(len(avg_predictions))\n",
    "    }\n",
    "\n",
    "    return sentiment_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TiÃªu cá»±c': 0.047979678958654404,\n",
       " 'TÃ­ch cá»±c': 0.12986205518245697,\n",
       " 'Trung tÃ­nh': 0.8221582174301147}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\" Trong má»™t cuá»™c há»p bÃ¡o diá»…n ra vÃ o sÃ¡ng nay (22/3), Ã´ng Nguyá»…n Äá»©c Chung, Chá»§ tá»‹ch UBND TP HÃ  Ná»™i, cho biáº¿t Ä‘Ã£ Ä‘á» xuáº¥t Bá»™ Y táº¿ cáº¥p 1 triá»‡u liá»u vaccine ngá»«a COVID-19 cho HÃ  Ná»™i.\"\"\"\n",
    "predict_sentiment_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LAP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'NEG': 0.6783797740936279, 'POS': 0.03750285878777504, 'NEU': 0.2841174304485321}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from underthesea import word_tokenize  # DÃ¹ng Ä‘á»ƒ word-segmented vÄƒn báº£n\n",
    "\n",
    "# Load mÃ´ hÃ¬nh PhoBERT Ä‘Ã£ fine-tuned cho phÃ¢n tÃ­ch cáº£m xÃºc\n",
    "checkpoint = \"wonrax/phobert-base-vietnamese-sentiment\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Load tokenizer cá»§a PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=False)\n",
    "\n",
    "def predict_sentiment_score_wonrax(text, max_length=256, stride=50):\n",
    "    \"\"\"\n",
    "    Dá»± Ä‘oÃ¡n Ä‘iá»ƒm cáº£m xÃºc cá»§a vÄƒn báº£n dÃ i báº±ng PhoBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): VÄƒn báº£n cáº§n phÃ¢n tÃ­ch.\n",
    "        max_length (int): Sá»‘ token tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh.\n",
    "        stride (int): Má»©c Ä‘á»™ chá»“ng láº¥n giá»¯a cÃ¡c Ä‘oáº¡n.\n",
    "\n",
    "    Returns:\n",
    "        dict: XÃ¡c suáº¥t cá»§a cÃ¡c nhÃ£n cáº£m xÃºc {label: probability}\n",
    "    \"\"\"\n",
    "    # BÆ°á»›c 1: TÃ¡ch tá»« báº±ng Underthesea Ä‘á»ƒ phÃ¹ há»£p vá»›i PhoBERT\n",
    "    word_segmented_text = word_tokenize(text, format=\"text\")\n",
    "\n",
    "    # BÆ°á»›c 2: Tokenize vÄƒn báº£n dÃ i, cáº¯t thÃ nh tá»«ng Ä‘oáº¡n nhá»\n",
    "    inputs = tokenizer(\n",
    "        word_segmented_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    input_keys = [\"input_ids\", \"attention_mask\"]\n",
    "    all_predictions = []\n",
    "\n",
    "    # BÆ°á»›c 3: Cháº¡y mÃ´ hÃ¬nh trÃªn tá»«ng Ä‘oáº¡n nhá»\n",
    "    with torch.no_grad():  # KhÃ´ng tÃ­nh gradient Ä‘á»ƒ tÄƒng tá»‘c\n",
    "        for i in range(len(inputs[\"input_ids\"])):  # Duyá»‡t qua tá»«ng Ä‘oáº¡n Ä‘Ã£ cáº¯t\n",
    "            segment = {k: inputs[k][i].unsqueeze(0) for k in input_keys}\n",
    "            outputs = model(**segment)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_predictions.append(predictions[0].tolist())\n",
    "\n",
    "    # BÆ°á»›c 4: TÃ­nh trung bÃ¬nh dá»± Ä‘oÃ¡n tá»« táº¥t cáº£ cÃ¡c Ä‘oáº¡n\n",
    "    avg_predictions = torch.tensor(all_predictions).mean(dim=0)\n",
    "\n",
    "    # BÆ°á»›c 5: Chuyá»ƒn káº¿t quáº£ thÃ nh dict vá»›i nhÃ£n vÃ  xÃ¡c suáº¥t tÆ°Æ¡ng á»©ng\n",
    "    sentiment_scores = {\n",
    "        model.config.id2label[j]: avg_predictions[j].item()\n",
    "        for j in range(len(avg_predictions))\n",
    "    }\n",
    "\n",
    "    return sentiment_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'NEG': 0.868488073348999, 'POS': 0.02748456411063671, 'NEU': 0.10402730852365494}\n"
     ]
    }
   ],
   "source": [
    "# Test thá»­ vá»›i vÄƒn báº£n dÃ i\n",
    "text = \"\"\"ThÃ´ng tin Tá»•ng thá»‘ng Má»¹ Donald Trump Ä‘Æ°a ra lá»‡nh Ã¡p thuáº¿ Ä‘á»‘i á»©ng vá»›i Viá»‡t Nam lÃªn tá»›i 46% lÃ m thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam chao Ä‘áº£o.\n",
    "\"\"\"\n",
    "result = predict_sentiment_score_wonrax(text)\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"Sentiment Scores:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"filtered_cafef_news_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_sentiment_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m text_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVN-Index tÄƒng 100 Ä‘iá»ƒm.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m      3\u001b[0m text_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVN-Index tÄƒng 500 Ä‘iá»ƒm.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_sentiment_score\u001b[49m(text_1))  \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_sentiment_score(text_2))  \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_sentiment_score(text_3))  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_sentiment_score' is not defined"
     ]
    }
   ],
   "source": [
    "text_1 = \"\"\"Má»¹ Ä‘Ã¡nh thuáº¿ 46% vá»›i Viá»‡t Nam, cao nháº¥t trong sá»‘ cÃ¡c nÆ°á»›c bá»‹ Ã¡p thuáº¿.\"\"\" \n",
    "text_2 = \"VN-Index tÄƒng 100 Ä‘iá»ƒm.\"  \n",
    "text_3 = \"VN-Index tÄƒng 500 Ä‘iá»ƒm.\"  \n",
    "\n",
    "print(predict_sentiment_score(text_1))  \n",
    "print(predict_sentiment_score(text_2))  \n",
    "print(predict_sentiment_score(text_3))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"TIN Má»šIThá»‹ trÆ°á»ng diá»…n ra giáº±ng co vÃ  lÃ¬nh xÃ¬nh quanh tham chiáº¿u trong suá»‘t phiÃªn giao dá»‹ch. Cáº£ bÃªn bÃ¡n vÃ  bÃªn mua Ä‘á»u dÃ¨ chá»«ng khi VN-Index cÃ³ chuá»—i phiÃªn tÄƒng liÃªn tá»¥c vÃ  Ä‘ang á»Ÿ trÃªn vÃ¹ng giÃ¡ cao. Báº¥t ngá» xáº£y ra vÃ o cuá»‘i phiÃªn khi bÃªn bÃ¡n xáº£ hÃ ng máº¡nh, khiáº¿n Ä‘Ã  bÃ¡n lan rá»™ng ra cÃ¡c nhÃ³m ngÃ nh.Cá»• phiáº¿u    dáº§u khÃ­ trá»Ÿ thÃ nh nhÃ³m giáº£m máº¡nh nháº¥t khi BSR, PVS, PVD, PLX, PVB... Ä‘á»u nhuá»‘m Ä‘á». NhÃ³m cá»• phiáº¿u Ä‘áº§u tÆ° cÃ´ng bá»‘c hÆ¡i Ä‘Ã¡ng ká»ƒ khi, VCG giáº£m 5,4%, HHV giáº£m 3,5%, LCG giáº£m 3,7%, CTD giáº£m 4,6%, HCB giáº£m 4,2%, FCN giáº£m 3,8%...Ãp lá»±c chá»‘t lá»i cÅ©ng diá»…n ra á»Ÿ nhÃ³m     cá»• phiáº¿u       ngÃ¢n hÃ ng    khi MBB giáº£m 1,2%, TCB giáº£m 2%, VPB giáº£m 1,8%, HDB giáº£m 2,1%, MSB giáº£m 2,1%, VCB giáº£m 1,8%. Nhá»¯ng mÃ£ cá»• phiáº¿u ngÃ¢n hÃ ng nÃ y cÅ©ng táº¡o sá»©c Ã©p lá»›n nháº¥t Ä‘áº¿n chá»‰ sá»‘ VN-Index hÃ´m nay.Cá»• phiáº¿u     báº¥t Ä‘á»™ng sáº£n    cÃ³ sá»± phÃ¢n hÃ³a. DÃ¹ sá»‘ mÃ£ giáº£m Ä‘iá»ƒm gáº¥p 3 láº§n sá»‘ mÃ£ tÄƒng nhÆ°ng nhá» sá»± gÃ¡nh Ä‘á»¡ tá»« cá»• phiáº¿u 'nhÃ \"     Vingroup    , vá» tá»•ng thá»ƒ nhÃ³m cá»• phiáº¿u nÃ y váº«n tÄƒng Ä‘iá»ƒm. Trong Ä‘Ã³, VRE tÄƒng 2,7%, VHM tÄƒng 1,8%, Ä‘áº·c biá»‡tVICtÄƒng tráº§n lÃªn 51.400 Ä‘á»“ng/cá»• phiáº¿u, má»©c cao nháº¥t trong vÃ²ng 18 thÃ¡ng trá»Ÿ láº¡i Ä‘Ã¢y. Chá»‰ riÃªng VIC Ä‘Ã£ Ä‘Ã³ng gÃ³p tá»›i hÆ¡n 3 Ä‘iá»ƒm vÃ o chá»‰ sá»‘ VN-Index giÃºp thá»‹ trÆ°á»ng bá»›t Ä‘Ã  giáº£m.Sáº¯c Ä‘á» lan rá»™ng trong phiÃªn hÃ´m nay (13/3).Káº¿t thÃºc phiÃªn giao dá»‹ch, VN-Index giáº£m 8,14 Ä‘iá»ƒm vá» 1.326,2 Ä‘iá»ƒm; HNX-Index giáº£m 0,56 Ä‘iá»ƒm cÃ²n 241,3 Ä‘iá»ƒm, Upcom giáº£m 0,43 Ä‘iá»ƒm, cÃ²n 98,8 Ä‘iá»ƒm. Äá»™ rá»™ng thá»‹ trÆ°á»ng nghiÃªng hoÃ n toÃ n vá» sáº¯c Ä‘á» vá»›i 303 mÃ£ giáº£m, vÃ  91 mÃ£ tÄƒng. Sá»‘ mÃ£ giáº£m cÅ©ng chiáº¿m Ä‘Ã¡ng ká»ƒ bÃªn rá»• VN30 vá»›i 19 mÃ£, so vá»›i 9 mÃ£ tÄƒng vÃ  2 mÃ£ Ä‘i ngang.Thanh khoáº£n trong phiÃªn hÃ´m nay Ä‘Æ°á»£c cáº£i thiá»‡n vá»›i hÆ¡n 1 tá»· cá»• phiáº¿u Ä‘Æ°á»£c trao tay, tÆ°Æ¡ng Ä‘Æ°Æ¡ng giÃ¡ trá»‹ giao dá»‹ch Ä‘áº¡t gáº§n 24.500 tá»· Ä‘á»“ng.ÄÃ¡ng chÃº Ã½, khi khá»‘i ná»™i xáº£ hÃ ng, khá»‘i ngoáº¡i trong phiÃªn hÃ´m nay láº¡i mua khÃ¡ tÃ­ch cá»±c. Khá»‘i nÃ y mua vÃ o lÆ°á»£ng cá»• phiáº¿u trá»‹ giÃ¡ 2.522 tá»· Ä‘á»“ng vÃ  bÃ¡n ra 2.370 tá»· Ä‘á»“ng trÃªn sÃ n HoSE.Theo thá»‘ng kÃª, 2 cá»• phiáº¿u Ä‘Æ°á»£c nhÃ  Ä‘áº§u tÆ° ngoáº¡i gom máº¡nh lÃ  VJC vÃ  SSI PhÃ­a ngÆ°á»£c láº¡i, khá»‘i ngoáº¡i bÃ¡n rÃ²ng hÆ¡n 100 tá»· Ä‘á»“ng 2 mÃ£ VCB vÃ  VNM.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äoáº¡n vÄƒn má»›i chá»©a cÃ¡c cÃ¢u lá»c theo tá»« khÃ³a:\n",
      "Cá»• phiáº¿u    dáº§u khÃ­ trá»Ÿ thÃ nh nhÃ³m giáº£m máº¡nh nháº¥t khi BSR, PVS, PVD, PLX, PVB.\n",
      "ğŸ” CÃ¡c cÃ¢u chá»©a tá»« khÃ³a:\n",
      "- Cá»• phiáº¿u    dáº§u khÃ­ trá»Ÿ thÃ nh nhÃ³m giáº£m máº¡nh nháº¥t khi BSR, PVS, PVD, PLX, PVB\n"
     ]
    }
   ],
   "source": [
    "# Danh sÃ¡ch tá»« khÃ³a cáº§n lá»c\n",
    "keywords = [\"dáº§u khÃ­\",'PLX']\n",
    "\n",
    "# TÃ¡ch cÃ¢u theo dáº¥u cháº¥m, Ä‘áº£m báº£o khÃ´ng lÃ m vá»¡ sá»‘ tháº­p phÃ¢n\n",
    "sentences = re.split(r'(?<!\\d)\\.(?!\\d)', text)\n",
    "\n",
    "# Loáº¡i bá» khoáº£ng tráº¯ng thá»«a vÃ  cÃ¢u rá»—ng\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Lá»c cÃ¢u chá»©a báº¥t ká»³ tá»« khÃ³a nÃ o trong danh sÃ¡ch\n",
    "filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "# Ná»‘i cÃ¡c cÃ¢u thÃ nh má»™t Ä‘oáº¡n vÄƒn má»›i\n",
    "filtered_text = \". \".join(filtered_sentences) + \".\"\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"ğŸ” Äoáº¡n vÄƒn má»›i chá»©a cÃ¡c cÃ¢u lá»c theo tá»« khÃ³a:\")\n",
    "print(filtered_text)\n",
    "# In káº¿t quáº£\n",
    "print(\"ğŸ” CÃ¡c cÃ¢u chá»©a tá»« khÃ³a:\")\n",
    "for i, sentence in enumerate(filtered_sentences, 1):\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"cafef_news_details.csv\")\n",
    "# Danh sÃ¡ch tá»« khÃ³a cáº§n lá»c\n",
    "keywords = [\"opec\"]\n",
    "\n",
    "# HÃ m xá»­ lÃ½ lá»c cÃ¢u chá»©a tá»« khÃ³a\n",
    "def filter_sentences(text, keywords):\n",
    "    if pd.isna(text):  # Kiá»ƒm tra náº¿u giÃ¡ trá»‹ NaN\n",
    "        return \"\"\n",
    "    \n",
    "    # TÃ¡ch cÃ¢u theo dáº¥u cháº¥m, giá»¯ nguyÃªn sá»‘ tháº­p phÃ¢n\n",
    "    sentences = re.split(r'(?<!\\d)\\.(?!\\d)', text)\n",
    "    \n",
    "    # Loáº¡i bá» khoáº£ng tráº¯ng vÃ  cÃ¢u rá»—ng\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    # Lá»c cÃ¡c cÃ¢u chá»©a tá»« khÃ³a\n",
    "    filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "\n",
    "    # Ná»‘i láº¡i thÃ nh Ä‘oáº¡n vÄƒn báº£n\n",
    "    return \". \".join(filtered_sentences) + \".\" if filtered_sentences else \"\"\n",
    "\n",
    "# Ãp dá»¥ng hÃ m vÃ o cá»™t má»›i\n",
    "df[\"filtered_content\"] = df[\"content\"].apply(lambda x: filter_sentences(x, keywords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiá»ƒn thá»‹ káº¿t quáº£\n",
    "df[[\"content\", \"filtered_content\"]].to_csv(\"test_processing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fil = pd.read_csv(\"test_processing.csv\")\n",
    "# Lá»c cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ trong cá»™t \"filtered_content\" (loáº¡i bá» NaN)\n",
    "df_fil_filtered = df_fil.dropna(subset=[\"filtered_content\"])\n",
    "\n",
    "df_fil_filtered.to_csv(\"test_processing.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
