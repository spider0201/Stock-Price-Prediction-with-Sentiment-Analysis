{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model và tokenizer\n",
    "checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "def predict_sentiment_score(text, max_length=256, stride=50):\n",
    "    \"\"\"\n",
    "    Dự đoán điểm cảm xúc của văn bản dài bằng PhoBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): Văn bản cần phân tích.\n",
    "        max_length (int): Số token tối đa của mô hình.\n",
    "        stride (int): Mức độ chồng lấn giữa các đoạn.\n",
    "\n",
    "    Returns:\n",
    "        dict: Xác suất của các nhãn cảm xúc {label: probability}\n",
    "    \"\"\"\n",
    "    # Tokenize văn bản dài, cắt thành từng đoạn\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    input_keys = [\"input_ids\", \"attention_mask\"]\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Không tính gradient để tăng tốc\n",
    "        for i in range(len(inputs[\"input_ids\"])):  # Duyệt qua từng đoạn đã cắt\n",
    "            segment = {k: inputs[k][i].unsqueeze(0) for k in input_keys}\n",
    "            outputs = model(**segment)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_predictions.append(predictions[0].tolist())\n",
    "\n",
    "    # Tính trung bình dự đoán từ tất cả các đoạn\n",
    "    avg_predictions = torch.tensor(all_predictions).mean(dim=0)\n",
    "\n",
    "    # Chuyển kết quả thành dict với nhãn và xác suất tương ứng\n",
    "    sentiment_scores = {\n",
    "        model.config.id2label[j]: avg_predictions[j].item()\n",
    "        for j in range(len(avg_predictions))\n",
    "    }\n",
    "\n",
    "    return sentiment_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tiêu cực': 0.047979678958654404,\n",
       " 'Tích cực': 0.12986205518245697,\n",
       " 'Trung tính': 0.8221582174301147}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\" Trong một cuộc họp báo diễn ra vào sáng nay (22/3), ông Nguyễn Đức Chung, Chủ tịch UBND TP Hà Nội, cho biết đã đề xuất Bộ Y tế cấp 1 triệu liều vaccine ngừa COVID-19 cho Hà Nội.\"\"\"\n",
    "predict_sentiment_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LAP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'NEG': 0.6783797740936279, 'POS': 0.03750285878777504, 'NEU': 0.2841174304485321}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from underthesea import word_tokenize  # Dùng để word-segmented văn bản\n",
    "\n",
    "# Load mô hình PhoBERT đã fine-tuned cho phân tích cảm xúc\n",
    "checkpoint = \"wonrax/phobert-base-vietnamese-sentiment\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Load tokenizer của PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=False)\n",
    "\n",
    "def predict_sentiment_score_wonrax(text, max_length=256, stride=50):\n",
    "    \"\"\"\n",
    "    Dự đoán điểm cảm xúc của văn bản dài bằng PhoBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): Văn bản cần phân tích.\n",
    "        max_length (int): Số token tối đa của mô hình.\n",
    "        stride (int): Mức độ chồng lấn giữa các đoạn.\n",
    "\n",
    "    Returns:\n",
    "        dict: Xác suất của các nhãn cảm xúc {label: probability}\n",
    "    \"\"\"\n",
    "    # Bước 1: Tách từ bằng Underthesea để phù hợp với PhoBERT\n",
    "    word_segmented_text = word_tokenize(text, format=\"text\")\n",
    "\n",
    "    # Bước 2: Tokenize văn bản dài, cắt thành từng đoạn nhỏ\n",
    "    inputs = tokenizer(\n",
    "        word_segmented_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    input_keys = [\"input_ids\", \"attention_mask\"]\n",
    "    all_predictions = []\n",
    "\n",
    "    # Bước 3: Chạy mô hình trên từng đoạn nhỏ\n",
    "    with torch.no_grad():  # Không tính gradient để tăng tốc\n",
    "        for i in range(len(inputs[\"input_ids\"])):  # Duyệt qua từng đoạn đã cắt\n",
    "            segment = {k: inputs[k][i].unsqueeze(0) for k in input_keys}\n",
    "            outputs = model(**segment)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_predictions.append(predictions[0].tolist())\n",
    "\n",
    "    # Bước 4: Tính trung bình dự đoán từ tất cả các đoạn\n",
    "    avg_predictions = torch.tensor(all_predictions).mean(dim=0)\n",
    "\n",
    "    # Bước 5: Chuyển kết quả thành dict với nhãn và xác suất tương ứng\n",
    "    sentiment_scores = {\n",
    "        model.config.id2label[j]: avg_predictions[j].item()\n",
    "        for j in range(len(avg_predictions))\n",
    "    }\n",
    "\n",
    "    return sentiment_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'NEG': 0.868488073348999, 'POS': 0.02748456411063671, 'NEU': 0.10402730852365494}\n"
     ]
    }
   ],
   "source": [
    "# Test thử với văn bản dài\n",
    "text = \"\"\"Thông tin Tổng thống Mỹ Donald Trump đưa ra lệnh áp thuế đối ứng với Việt Nam lên tới 46% làm thị trường chứng khoán Việt Nam chao đảo.\n",
    "\"\"\"\n",
    "result = predict_sentiment_score_wonrax(text)\n",
    "\n",
    "# In kết quả\n",
    "print(\"Sentiment Scores:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"filtered_cafef_news_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_sentiment_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m text_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVN-Index tăng 100 điểm.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m      3\u001b[0m text_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVN-Index tăng 500 điểm.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_sentiment_score\u001b[49m(text_1))  \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_sentiment_score(text_2))  \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_sentiment_score(text_3))  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_sentiment_score' is not defined"
     ]
    }
   ],
   "source": [
    "text_1 = \"\"\"Mỹ đánh thuế 46% với Việt Nam, cao nhất trong số các nước bị áp thuế.\"\"\" \n",
    "text_2 = \"VN-Index tăng 100 điểm.\"  \n",
    "text_3 = \"VN-Index tăng 500 điểm.\"  \n",
    "\n",
    "print(predict_sentiment_score(text_1))  \n",
    "print(predict_sentiment_score(text_2))  \n",
    "print(predict_sentiment_score(text_3))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"TIN MỚIThị trường diễn ra giằng co và lình xình quanh tham chiếu trong suốt phiên giao dịch. Cả bên bán và bên mua đều dè chừng khi VN-Index có chuỗi phiên tăng liên tục và đang ở trên vùng giá cao. Bất ngờ xảy ra vào cuối phiên khi bên bán xả hàng mạnh, khiến đà bán lan rộng ra các nhóm ngành.Cổ phiếu    dầu khí trở thành nhóm giảm mạnh nhất khi BSR, PVS, PVD, PLX, PVB... đều nhuốm đỏ. Nhóm cổ phiếu đầu tư công bốc hơi đáng kể khi, VCG giảm 5,4%, HHV giảm 3,5%, LCG giảm 3,7%, CTD giảm 4,6%, HCB giảm 4,2%, FCN giảm 3,8%...Áp lực chốt lời cũng diễn ra ở nhóm     cổ phiếu       ngân hàng    khi MBB giảm 1,2%, TCB giảm 2%, VPB giảm 1,8%, HDB giảm 2,1%, MSB giảm 2,1%, VCB giảm 1,8%. Những mã cổ phiếu ngân hàng này cũng tạo sức ép lớn nhất đến chỉ số VN-Index hôm nay.Cổ phiếu     bất động sản    có sự phân hóa. Dù số mã giảm điểm gấp 3 lần số mã tăng nhưng nhờ sự gánh đỡ từ cổ phiếu 'nhà\"     Vingroup    , về tổng thể nhóm cổ phiếu này vẫn tăng điểm. Trong đó, VRE tăng 2,7%, VHM tăng 1,8%, đặc biệtVICtăng trần lên 51.400 đồng/cổ phiếu, mức cao nhất trong vòng 18 tháng trở lại đây. Chỉ riêng VIC đã đóng góp tới hơn 3 điểm vào chỉ số VN-Index giúp thị trường bớt đà giảm.Sắc đỏ lan rộng trong phiên hôm nay (13/3).Kết thúc phiên giao dịch, VN-Index giảm 8,14 điểm về 1.326,2 điểm; HNX-Index giảm 0,56 điểm còn 241,3 điểm, Upcom giảm 0,43 điểm, còn 98,8 điểm. Độ rộng thị trường nghiêng hoàn toàn về sắc đỏ với 303 mã giảm, và 91 mã tăng. Số mã giảm cũng chiếm đáng kể bên rổ VN30 với 19 mã, so với 9 mã tăng và 2 mã đi ngang.Thanh khoản trong phiên hôm nay được cải thiện với hơn 1 tỷ cổ phiếu được trao tay, tương đương giá trị giao dịch đạt gần 24.500 tỷ đồng.Đáng chú ý, khi khối nội xả hàng, khối ngoại trong phiên hôm nay lại mua khá tích cực. Khối này mua vào lượng cổ phiếu trị giá 2.522 tỷ đồng và bán ra 2.370 tỷ đồng trên sàn HoSE.Theo thống kê, 2 cổ phiếu được nhà đầu tư ngoại gom mạnh là VJC và SSI Phía ngược lại, khối ngoại bán ròng hơn 100 tỷ đồng 2 mã VCB và VNM.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Đoạn văn mới chứa các câu lọc theo từ khóa:\n",
      "Cổ phiếu    dầu khí trở thành nhóm giảm mạnh nhất khi BSR, PVS, PVD, PLX, PVB.\n",
      "🔎 Các câu chứa từ khóa:\n",
      "- Cổ phiếu    dầu khí trở thành nhóm giảm mạnh nhất khi BSR, PVS, PVD, PLX, PVB\n"
     ]
    }
   ],
   "source": [
    "# Danh sách từ khóa cần lọc\n",
    "keywords = [\"dầu khí\",'PLX']\n",
    "\n",
    "# Tách câu theo dấu chấm, đảm bảo không làm vỡ số thập phân\n",
    "sentences = re.split(r'(?<!\\d)\\.(?!\\d)', text)\n",
    "\n",
    "# Loại bỏ khoảng trắng thừa và câu rỗng\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Lọc câu chứa bất kỳ từ khóa nào trong danh sách\n",
    "filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "# Nối các câu thành một đoạn văn mới\n",
    "filtered_text = \". \".join(filtered_sentences) + \".\"\n",
    "\n",
    "# In kết quả\n",
    "print(\"🔎 Đoạn văn mới chứa các câu lọc theo từ khóa:\")\n",
    "print(filtered_text)\n",
    "# In kết quả\n",
    "print(\"🔎 Các câu chứa từ khóa:\")\n",
    "for i, sentence in enumerate(filtered_sentences, 1):\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"cafef_news_details.csv\")\n",
    "# Danh sách từ khóa cần lọc\n",
    "keywords = [\"opec\"]\n",
    "\n",
    "# Hàm xử lý lọc câu chứa từ khóa\n",
    "def filter_sentences(text, keywords):\n",
    "    if pd.isna(text):  # Kiểm tra nếu giá trị NaN\n",
    "        return \"\"\n",
    "    \n",
    "    # Tách câu theo dấu chấm, giữ nguyên số thập phân\n",
    "    sentences = re.split(r'(?<!\\d)\\.(?!\\d)', text)\n",
    "    \n",
    "    # Loại bỏ khoảng trắng và câu rỗng\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    # Lọc các câu chứa từ khóa\n",
    "    filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "\n",
    "    # Nối lại thành đoạn văn bản\n",
    "    return \". \".join(filtered_sentences) + \".\" if filtered_sentences else \"\"\n",
    "\n",
    "# Áp dụng hàm vào cột mới\n",
    "df[\"filtered_content\"] = df[\"content\"].apply(lambda x: filter_sentences(x, keywords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị kết quả\n",
    "df[[\"content\", \"filtered_content\"]].to_csv(\"test_processing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fil = pd.read_csv(\"test_processing.csv\")\n",
    "# Lọc các dòng có giá trị trong cột \"filtered_content\" (loại bỏ NaN)\n",
    "df_fil_filtered = df_fil.dropna(subset=[\"filtered_content\"])\n",
    "\n",
    "df_fil_filtered.to_csv(\"test_processing.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
